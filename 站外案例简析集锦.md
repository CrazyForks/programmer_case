# 收藏各种案例及链接--持续更新中……

## 目的

收藏各种案例及链接，但是总是没法保证链接的有效性，也有一些因为其它原因缺失的原始链接，我会尽量收集他们的链接

## Nginx和内核案例

### [从 Nginx 流量下降探秘内核收包机制](https://mp.weixin.qq.com/s/70ARIx-JzXwTDe0a4vg_ig)

主要是收发包hash后队列不均衡

listen端口的socket因为还没有四元组，默认在一个很小(size 32)的hash表中查找，如果listen端口过多，典型的是开启reuseport后一个核在同一个listen port上都有一个socket，造成这个socket列表被放大很多倍，导致hash表冲突严重，容易在hash表查找上消耗比较高的CPU(4.17开始修复了这个问题)

注意ipip、overlay等虚拟网络导致的连接hash不均匀，比如vxlan的udp端口固定，所有连接都hash到一个核上

> 从网卡厂商处进一步确认得知，我们在使用的这款网卡，是不支持解析封装后的数据包的,只会以外层IP作为哈希依据。厂商提供了一款新型号的网卡，是支持解析IPIP及GRE内层IP PORT的。我们经过实测这两种网卡，发现确实如此。
> 
> 内核RPS函数中默认只识别GRE_VERSION=0的协议并识别嵌套的四元组再hash，如果是 GRE_VERSION=1的GRE数据包则按外层四元组hash(导致了队列不均衡)
> 
> 内核在 Linux 4.10之后，支持PPTP协议包的RPS负载均衡。


```
//默认listen socket hash表的大小
#define INET_LHTABLE_SIZE  32 

//建议设为 on
#ethtool -k bond0 |grep hash
receive-hashing: off [fixed]

//查看每个队列收发包是否均衡
#ethtool -S eth1 | grep rx_queue | grep packets
     rx_queue_0_packets: 720836
     rx_queue_1_packets: 441534
     rx_queue_2_packets: 96930
     rx_queue_3_packets: 197180
     rx_queue_4_packets: 158823
     rx_queue_5_packets: 144344
     rx_queue_6_packets: 154652
     rx_queue_7_packets: 158965
     rx_queue_8_packets: 506354

//对于RSS网卡多队列已经是均衡的机器，让RPS直接使用网卡硬件哈希值，由于硬件哈希值足够分散，因此RPS效果也是均衡的。关掉rps hash
kernel.rps_ignore_l4_rxhash = 0

```

![Image](/Users/ren/src/blog/951413iMgBlog/640-8259052.jpeg)

![Image](/Users/ren/src/blog/951413iMgBlog/640-20221112211745593.jpeg)

#### RSS多队列导致PMTU的问题

ALB服务器机器使用的intel网卡, 有个硬件RSS(Receive Side Scaling)特性, RSS是一个通过数据包的元组信息将数据包散列到不同网卡队列的功能, 用于将收到的报文分发到多个描述符队列, 那么后续程序可以用不同CPU核心处理不同队列, 以达到负载分担的效果.
RSS原理是使用一个key来对ip地址,端口号等字段进行hash计算, 再用hash值对队列数取模算出对应的队列, 报文放入此队列后, 队列绑定了cpu, 因此可以相同元组(源目的地址)的报文会分配到相同cpu上处理.
老版本使用的是vxlan外层报文进行RSS, 在新版本切到了内层RSS; 这两个实例是落在两个不同集群, 正常的实例还是老版本用的外层RSS, 对于GRE代理访问模式没有问题; 新版本用的内层RSS, 看到的源地址是192.168.0.64, 但实际发icmp包的是gre那台ecs(1.1.1.51), 所以icmp跟session按内层rss策略落不到一个核去了，所以后端服务器无法收到ICMP报文，从而无法自动调整报文MSS大小。
简单说, 就是gre代理回icmp的这种场景, 在内层rss版本上不支持了。

![image-20221125164254479](/Users/ren/src/blog/951413iMgBlog/image-20221125164254479.png)

上图为客户端与服务端交互的报文，可以看到当服务端发送1460 payload的报文的时候，经过中间的ECS时，ECS检测到包超过ECS的MTU后会返回一个ICMP报文，此ICMP报文作用是告诉服务端，ECS的链路MTU只有1476（因为做Gre、vxlan占用了部分包头，所以MTU不是常见的1500），当服务端收到这个ICMP报文的时候，服务端就会知道中间链路只能允许payload 1436的报文通过，自然就会缩小发送的mss大小。

![image-20221125164508492](/Users/ren/src/blog/951413iMgBlog/image-20221125164508492.png)

### [由Nginx SY CPU高负载引发内核探索之旅](https://mp.weixin.qq.com/s/njpdTW5TndO4-H7nbEpXAA)

local port 不够的时候inet_hash_connect 中的spin_lock 会消耗过高的 sys（特别注意4.14内核后 local port 分奇偶数，每次loop+2，所以更容易触发port不够的场景）

![Image](/Users/ren/src/blog/951413iMgBlog/640-8259033.png)

![Image](/Users/ren/src/blog/951413iMgBlog/640-20221112211814567.png)

### 一次Nginx卡顿 -- 历经三年的分析过程

从2018- 9.1 号开始，LVS同学反馈存储线上集群经常掉机器，健康检查失败，同集群的用户也反馈上传下载会出现偶发性的“超慢”请求，请求无应答，影响用户业务。此情况持续了几天，每天都有少量出现。而只有这个集群出现，其他压力更大的集群皆未出现.

现象：

在8.15 号左右升级nginx 配置，打开了reuse_port 功能

在压力大的后端服务环境不容易出现

关闭reuse_port后 问题少了很多

从存储入口 nginx 日志上看，所有**失败的健康检查请求都是0ms 的499 错误码**（健康检查设置超时是2秒），但实际出问题的时候有5s-2分钟没有任何日志输出（Nginx卡了这么久）-- 要么是Nginx卡住没去accept，要么是accept了没响应

**所有超时来自同一个worker**

以上现象推定是某个worker卡住了，通过抓堆栈脚本

```
cat monitor_nginx_stack.sh 

echo "" > nginx-stack
for i in {1..100000}; do 
    curl --connect-timeout 1 -m 1 localhost
    ret=$?
        if [ "x$ret" != "x0" ]; then
        echo "curl timeout..."
        date >> nginx-stack
        for i in `ps aux | grep "nginx: worker process"|grep -v "grep"| awk '{print $2}'`; do 
            echo $i >> nginx-stack
                pstack $i >> nginx-stack
        done
    fi
sleep 0.3
done

#一旦发现Nginx 有Hang 住迹象，就立即把所有Nginx Worker 栈抓出来（因为无法确认是哪个Worker Hang）
#再到日志中找到499 的curl localhost 请求，找到所在Worker 的PID, 确认Hang 住Worker 当前栈.

#注意上面的 curl --connect-timeout 1 -m 1 localhost
#--connect-timeout 这个是连接建立超时时间，光设置这个值还不能捕捉到hang，原因是连接还是能正常建立
#的，请求数据也能正常发（少量数据），所以还需要增加请求接收超时时间 -m 1 才能work.
```

抓到的栈确认是由于gzip 压缩导致，于是线下构建重现环境：上传2.7G的大text/xml文件，同时curl，压缩需要20s，这个过程 curl 不通，到此确认了这个问题

**结论**

> Nginx 对超大文件进行gzip，CPU 长时间未切出去，导致Nginx Hang 导致，其余事件无法处理.


**为什么打开reuse_port就会发生**

```
其实不打开reuse_port也会发生，只是发生的概率提高了而已.
在reuse_port off 的情况下，如果nginx worker hang住就不会抢accept锁去accept 新连接，其余worker可以处理在accept队列中的请求。但是正在处理的请求还是可能会hang.
在reuse_port on 的情况下，内核会分配好每个连接到哪个worker上处理，内核做均衡，此时如果某个worker hang住，链接就一直在accept 队列中得不到处理.
由于健康检查请求处理时间超级短，因此关闭reuse_port 后基本不会撞到.
```

**gzip 是流式的为什么CPU 切不出去**

```
    gzip 压缩确实是流式压缩的，但是由于Nginx 使用Epoll 是边缘触发，因此当套接字可读事件发生时一定要将套接字内数据读干净，否则Epoll 不会再次返回，在网络情况非常好及机器压力低的情况下

套接字持续有数据可读，造成这么一个循环：
读数据（未读完）->Gzip(需要时间，套接字又有数据过来)->读数据（未读完）->Gzip .....
因此造成CPU 持续切不出去.
```

**为什么压力更大的集群未出现**

```
集群压力大，后端压力大，后端喂给Nginx 的数据相对慢，因此gzip 流式压缩有机会切出去.
```

**为什么会是0ms的499**

```
    在accept 队列中的连接客户端已经超时断开了，在Nginx Hang 机结束后去accept 队列中取连接，并
读取连接上的数据，先读到请求行，开始进行处理，紧接着又读到连接上的RST 或者 FIN，Nginx 判断
为Client close premature，因此是个0ms 的499.
```

两年后的分析：

> 上面Nginx Hang死的原因是：Nginx 使用了边缘触发模式，因此Nginx 在套接字有可读性事件的情况下，必须把所有数据都读掉才行，在gzip buffer < connection rcvbuf 同时后端比较快时候，一次性读不完连接上所有数据，就会出现读数据->压缩->新数据到达->继续读数据-> 继续压缩... 的循环，由于压缩需要时间，此时套接字上又来了新的数据，只要数据来的速度比压缩的快，就会出现数据一直读不完的情况，CPU 就一直切不出去。
> 
> 解决：Nginx Server的 gzip_buffers 配置为 64*8k = 512K，给后端进程增加了设置sndbuf/rcvbuf 指令之后通过配置Nginx 与后 存储 server 之间的连接的rcvbuf 到512k 以内，这样就能解决这个问题了，实测这个修改几乎不影响后端整体吞吐，同时也不会出现Nginx worker Hang 的情况。


**worker hang住后连带灾难（计时失效，排查会复杂很多）**

rt 为什么是0ms，因为每个worker有一个 timer（不需要每次去读系统时间--效率差， 精度要求没那么高），worker卡住后 timer 也没有机会去跟新了，所以这时候记录的 rt 基本都不对了。同时 rt 为0 是因为请求一直在accept队列里，还没有开始计时（nginx accept后进入处理流程才开始计时），等到卡顿结束 nginx accept 时发现这个请求已经被客户端reset了（超时2秒）

Nginx 的时间是事件触发，异步刷新的，进程hang住，会影响时间更新。 和很多gc语言(对！我说的就是java)的工程师，为了优化 gettimeofday的性能(即使引入了vdso)，缓存时间戳，遇到gc的时候，客户端请求耗时N s，后台日志打印只耗费几十ms

从架构上思考：压缩也可以放到存储server，压缩好了给nginx，nginx不再进行压缩

​							  计时用统一一个worker，其它worker都来读取（设计、实现上的困难）

从问题到解决方案花了2年，还是对底层知识串联不够，如果当时知道是数据喂得太快的话就可以想到改tcp send buf之类的来绕过问题

#### 相关案例1

[Why does one NGINX worker take all the load?](https://blog.cloudflare.com/the-sad-state-of-linux-socket-balancing/)

[The story of one latency spike](https://blog.cloudflare.com/the-story-of-one-latency-spike/) 延时偶发性很高，用两次 ping 差异就搞定了抖动定位环境，然后再上 tcpdump，systemtap 分析内核函数是否卡顿，从30秒368万次 net_rx_action 调用都是1ms以内中发现最慢一次有23ms，再到 tcp_collapse 函数（avg 3ms, 最大21ms， 对应30秒总调用次数 155次）

The naive answer would go something along the lines of: the TCP receive buffer setting indicates the maximum number of bytes a `read()` syscall could retrieve without blocking.

每个packet包含data和metadata（包头，大概240 bytes），metadata 大小不会计入 receive buffer size, 极端情况下 data 非常小导致metadata占比高payload大，从而导致 receive buffer 不够，这时因为 buffer 紧张触发 tcp_collapse(it will merge adjacent TCP packets into one larger `sk_buff`), 很像GC过程，从而导致了 RT 的尖峰出现（net.ipv4.tcp_rmem = 4096 5242880 33554432）默认是5M，导致GC 卡顿久

解决方案：减小 rmem size（net.ipv4.tcp_rmem = 4096 1048576 2097152），修改后 [tcp_collapse](https://github.com/torvalds/linux/commit/b49960a05e32121d29316cfdf653894b88ac9190) 没有超过3ms的了，同时 net_rx_action 也从最大的23ms降低到了3ms

The solution at the time was to set tcp_rmem to 2 MiB, which minimizes the amount of time the kernel spends on TCP collapse processing.

[tcp_collapse 作用可以参考](https://github.com/torvalds/linux/commit/b49960a05e32121d29316cfdf653894b88ac9190)

> **When receiving frames, sk_rmem_alloc can hit sk_rcvbuf limit and we call tcp_prune_queue()/tcp_collapse() too often, especially when application is slow to drain its receive queue or in case of losses (netperf is fast, scp is slow). This is a major latency source.**


#### numa_balancing 导致的nginx卡顿

> 这个案例没有单独放，是因为和前一个卡顿现象是一样的，只是原因不一样，所以放一起分析


ECS 里面部署了Nginx，但是 Nginx一定低概率的记录健康检查失败或者499

![image-20220924091158877](/Users/ren/src/blog/951413iMgBlog/image-20220924091158877.png)

抓包对比从底层物理机和 ECS 抓，判断是 Nginx慢，但是一直没找到突然慢一下的原因，后面和系统的同学一起看 Nginx D住的时候都有numa_migrate,而且客户的机器的确是开了numa balance的，现在把所有的ingress迁移到不跨numa的节点问题没有在复现的

![image-20220924091212645](/Users/ren/src/blog/951413iMgBlog/image-20220924091212645.png)

这个场景是因为nginx 跨node访问内存过多，所以触发了内核的跨node迁移内存，必然造成卡顿

解决办法：nginx 绑定 node；或者关闭 numa_balancing

关于参数 numa_balancing：

> Enables/disables automatic page fault based NUMA memory balancing. Memory is moved automatically to nodes that access it often.
> 
> Enables/disables automatic NUMA memory balancing. On NUMA machines, there is a performance penalty if remote memory is accessed by a CPU. When this feature is enabled the kernel samples what task thread is accessing memory by periodically unmapping pages and later trapping a page fault. At the time of the page fault, it is determined if the data being accessed should be migrated to a local memory node.
> 
> The unmapping of pages and trapping faults incur additional overhead that ideally is offset by improved memory locality but there is no universal guarantee. If the target workload is already bound to NUMA nodes then this feature should be disabled. Otherwise, if the system overhead from the feature is too high then the rate the kernel samples for NUMA hinting faults may be controlled by the numa_balancing_scan_period_min_ms, numa_balancing_scan_delay_ms, numa_balancing_scan_period_max_ms, numa_balancing_scan_size_mb, and numa_balancing_settle_count sysctls.


## Java

### 深入 JVM 彻底剖析 ygc 越来越慢的原因(三大高手同台PK分析过程)

公众号两篇分析过程：https://mp.weixin.qq.com/s/IS5cb27Tmk_cIGm0tM9zVA（非源代码分析，更好的普适性）  http://lovestblog.cn/blog/2016/03/15/ygc-classloader/ (最后从源代码分析明确最终的原因，分析过程很赞)

https://www.infoq.cn/article/thorough-jvm-thorough-analysis-ygc-part01

#### 问题描述

下面这个case，YGCT会莫名其妙的变长，请解释其中的原因。

```
maven依赖
        <dependency>
            <groupId>com.thoughtworks.xstream</groupId>
            <artifactId>xstream</artifactId>
            <version>1.4.8</version>
        </dependency>

public class SlowYGC {
    public static void main(String[] args) throws Exception {
        while (true) {
            XStream xs = new XStream();
            xs.toString();
            xs = null;
        }
    }
}

jvm 参数 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xms512m -Xmx512m -Xmn100m -XX:+UseConcMarkSweepGC        
```

从以下日志可以看出YGC使用了ParNew这种并发回收器，YGCT从最初的0.0044060 secs 涨到了0.0397720 secs，在GC日志中还有一些需要注意的细节，比如每次YGC回收的空间都差不多，大致是从GC前的86Mb左右到GC后的5Mb，每次回收的空间差不多但是回收所用的时间却增加了10倍：

![image-20220706102446670](/Users/ren/src/blog/951413iMgBlog/image-20220706102446670.png)

#### 非源代码分析过程

增加full gc（其实通过 jmap 触发full gc也是一样的效果）

```java
public class SlowYGC {
    public static void main(String[] args) throws Exception {
        int i= 0;
        while (true) {
            XStream xs = new XStream();
            xs.toString();
            xs = null;
            if(i++ % 10000 == 0)
            {
                System.gc();
            }
        }
    }
}
```

可以看到full gc后 ygc也正常了（说明fgc干掉了一些东西，让ygc更快了）--这个尝试手段让问题原因更清晰一些了

![image-20220706113722680](/Users/ren/src/blog/951413iMgBlog/image-20220706113722680.png)

为了看到Full GC前后对象的回收情况，增加下面2个VM参数
-XX:+PrintClassHistogramBeforeFullGC -XX:+PrintClassHistogramAfterFullGC。
重新运行上面的代码，可以观察到下面的日志：

![image-20220706103130314](/Users/ren/src/blog/951413iMgBlog/image-20220706103130314.png)

上图左边是FGC前的对象情况，右边是FGC后的情况，结合我之前给出的GC Root候选人中的Monitor锁，相信你很快就找到20026个[Ljava.util.concurrent.ConcurrentHashMap$Segment对象几乎被全部回收了，ConcurrentHashMap中正是通过Segment来实现分段锁的。那什么逻辑会导致出现大量的Segment锁对象。继续看Full GC日志com.thoughtworks.xstream.core.util.CompositeClassLoader这个对象也差不多在FGC的时候全军覆没，所以怀疑是ClassLoader引起的锁竞争，下面在ClassLoader的源码中继续查找。

重现自定义ClassLoader导致YGCT 变长的问题：

```java
public class TestClassLoader4YGCT {
    public static void main(String[] args) throws Exception{
        while(true)
        {
            Object obj = newObject();
            obj.toString();
            obj = null;
        }
    }

    private static Object newObject() throws Exception
    {
        ClassLoader classLoader = new ClassLoader() {
            @Override
            public Class<?> loadClass(String s) throws ClassNotFoundException {
                try{
                    String fileName = s.substring(s.lastIndexOf(".") + 1)+ ".class";
                    InputStream inputStream = getClass().getResourceAsStream(fileName);
                    if(inputStream == null){
                        return super.loadClass(s);
                    }
                    byte[] b = new byte[inputStream.available()];
                    inputStream.read(b);
                    return defineClass(s,b,0,b.length);
                }catch (Exception e)
                {
                    System.out.println(e);
                    return null;
                }
            }
        };
        Class<?> obj = classLoader.loadClass("jvmstudy.classload.TestClassLoader4YGCT");
        return obj.newInstance();
    }
}
```

##### 结论

当大量new自定义的ClassLoader来加载时，会产生大量ClassLoader对象以及parallelLockMap（ConcurrentHashMap）对象，导致产生大量的Segment分段锁对象，大大增加了GC Roots的数量，导致YGC中的标记时间变长。如果能直接看到YGCT的详细使用情况，这个结论会显得更加严谨。

出问题时，CompositeClassLoader对象是位于gc root的Unreachable区的，同时可查看类加载器视图，所有的CompositeClassLoader对象(约19W)下未加载任何类；当时，young gc的时间由原来的0.04s已经增长到3s左右；

##### 相关信息

https://mail.openjdk.org/pipermail/hotspot-gc-use/2012-February/001087.html

#### [源码分析过程版本](http://lovestblog.cn/blog/2016/03/15/ygc-classloader/)

将XStream对象换成Object对象，会发现不存在这个问题，这个XStream的构造函数比较特殊，里面会创建很多的对象，不断构建各种大大小小的对象，一个XStream对象构建出来的时候大概好像有12M的样子。

可以通过禁掉XStream中部分代码来验证到底是哪个对象出了问题 -- 手段

比如将最后一个构造函数里的那些逻辑都禁掉，然后我们再跑测试看看还会不会让ygc不断恶化，最终我们会发现，如果我们直接使用如下构造函数构造对象时，如果传入的classloader是AppClassLoader，那么YGCT 就很稳定了。

```
 public static void main(String[] args) throws Exception {
        int i=0;
        while (true) {
            XStream xs = new XStream(null,null, new ClassLoaderReference(XStreamTest.class.getClassLoader()),null, new DefaultConverterLookup());
            xs.toString();
            xs=null;
        }
  }
```

到此说明和这个 ClassLoader 很相关

##### 类加载器原理

这里着重要说的两个概念是`初始类加载器`和`定义类加载器`。举个栗子说吧，AClassLoader->BClassLoader->CClassLoader，表示AClassLoader在加载类的时候会委托BClassLoader类加载器来加载，BClassLoader加载类的时候会委托CClassLoader来加载，假如我们使用AClassLoader来加载X这个类，而X这个类最终是被CClassLoader来加载的，那么我们称CClassLoader为X类的定义类加载器，而AClassLoader为X类的初始类加载器，JVM在加载某个类的时候对AClassLoader和CClassLoader进行记录，记录的数据结构是一个叫做SystemDictionary的hashtable，其key是根据ClassLoader对象和类名算出来的hash值（其实是一个entry，可以根据这个hash值找到具体的index位置，然后构建一个包含kalssName和classloader对象的entry放到map里），而value是真正的由定义类加载器加载的Klass对象，因为初始类加载器和定义类加载器是不同的classloader，因此算出来的hash值也是不同的，因此在SystemDictionary里会有多项值的value都是指向同一个Klass对象。

那么JVM为什么要分这两种类加载器呢，其实主要是为了快速找到已经加载的类，比如我们已经通过AClassLoader来触发了对X类的加载，当我们再次使用AClassLoader这个类加载器来加载X这个类的时候就不需要再委托给BClassLoader去找了，因为加载过的类在JVM里有这个类加载器的直接加载的记录，只需要直接返回对应的Klass对象即可。

当我们不断new XStream的时候会不断new CompositeClassLoader对象，加载类的时候会不断往SystemDictionary里插入记录，从而使SystemDictionary越来越膨胀，那自然而然会想到如果GC过程不断去扫描这个SystemDictionary的话，那随着SystemDictionary不断膨胀，那么GC的效率也就越低，抱着验证下猜想的方式我们可以使用perf工具来看看，如果发现cpu占比排前的函数如果都是操作SystemDictionary的，那就基本验证了我们的说法，下面是perf工具的截图，基本证实了这一点。

![Image](/Users/ren/src/blog/951413iMgBlog/640-7083886.jpeg)

##### 修改JVM代码加入 SH_PS_SystemDictionary_oops_do 耗时统计

```c
if (!_process_strong_tasks->is_task_claimed(SH_PS_SystemDictionary_oops_do)) {
    GCTraceTime t("SystemDictionary_OOPS_DO",PrintGCDetails,true,NULL); //新加代码
    if (so & SO_AllClasses) {
      SystemDictionary::oops_do(roots);
    } else if (so & SO_SystemClasses) {
      SystemDictionary::always_strong_oops_do(roots);
    }
  }
```

运行结果：

```
2016-03-14T23:57:24.293+0800: [GC2016-03-14T23:57:24.294+0800: [ParNew2016-03-14T23:57:24.296+0800: [SystemDictionary_OOPS_DO, 0.0578430 secs]
: 81920K->3184K(92160K), 0.0889740 secs] 81920K->3184K(514048K), 0.0900970 secs] [Times: user=0.27 sys=0.00, real=0.09 secs]
2016-03-14T23:57:28.467+0800: [GC2016-03-14T23:57:28.468+0800: [ParNew2016-03-14T23:57:28.468+0800: [SystemDictionary_OOPS_DO, 0.0779210 secs]
: 85104K->5175K(92160K), 0.1071520 secs] 85104K->5175K(514048K), 0.1080490 secs] [Times: user=0.65 sys=0.00, real=0.11 secs]
2016-03-14T23:57:32.984+0800: [GC2016-03-14T23:57:32.984+0800: [ParNew2016-03-14T23:57:32.984+0800: [SystemDictionary_OOPS_DO, 0.1075680 secs]
: 87095K->8188K(92160K), 0.1434270 secs] 87095K->8188K(514048K), 0.1439870 secs] [Times: user=0.90 sys=0.01, real=0.14 secs]
2016-03-14T23:57:37.900+0800: [GC2016-03-14T23:57:37.900+0800: [ParNew2016-03-14T23:57:37.901+0800: [SystemDictionary_OOPS_DO, 0.1745390 secs]
: 90108K->7093K(92160K), 0.2876260 secs] 90108K->9992K(514048K), 0.2884150 secs] [Times: user=1.44 sys=0.02, real=0.29 secs]
```

会发现YGC的时间变长的时候，SystemDictionary_OOPS_DO的时间也会相应变长多少

## 容器

### [No space left on device](https://www.manjusaka.blog/posts/2023/01/07/special-case-no-space-left-on-device/)

**OSError: [Errno 28] No space left on device**：

​	大部分时候不是真的磁盘没有空间了还有可能是inode不够了(df -ih 查看inode使用率)

​	尝试用 fallocate 来测试创建文件是否成功

​	尝试fdisk-l / tune2fs -l 来确认分区和文件系统的正确性

​	fallocate 创建一个文件名很长的文件失败(也就是原始报错的文件名)，同时fallocate 创建一个短文件名的文件成功

​	dmesg 查看系统报错信息

```
[13155344.231942] EXT4-fs warning (device sdd): ext4_dx_add_entry:2461: Directory (ino: 3145729) index full, reach max htree level :2
[13155344.231944] EXT4-fs warning (device sdd): ext4_dx_add_entry:2465: Large directory feature is not enabled on this filesystem
```

​	看起来是小文件太多撑爆了ext4的BTree索引，通过 tune2fs -l /dev/nvme1n1p1 验证下

```
#tune2fs -l /dev/nvme1n1p1 |grep Filesystem
Filesystem volume name:   /flash2
Filesystem revision #:    1 (dynamic)
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
Filesystem flags:         signed_directory_hash
Filesystem state:         clean
Filesystem OS type:       Linux
Filesystem created:       Fri Mar  6 17:08:36 2020
```

​	执行 `tune2fs -O large_dir ` /dev/nvme1n1p1 打开 large_dir 选项

```
tune2fs -l /dev/nvme1n1p1 |grep -i large
Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent flex_bg large_dir sparse_super large_file huge_file uninit_bg dir_nlink extra_isize
```

如上所示，开启后Filesystem features 多了 large_dir，[不过4.13以上内核才支持这个功能](https://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4.git/commit/?h=dev&id=88a399955a97fe58ddb2a46ca5d988caedac731b)

## 综合案例

### 我的黑色星期五（共5篇）

来自某个公号，具体见后述链接

#### 问题描述

8个NUMA节点，160个CPU，4TB内存。最明显的异常现象是1号NUMA节点的20个CPU比其它NUMA节点更繁忙，其中的8个CPU更是繁忙度高达100%，而且softirq软中断所占的比例在70%以上，1号NUMA节点的另外12个CPU虽然softirq不高但是usr%比其它NUMA节点的CPU高出很多。

中断主要来自一块网卡，因为网卡连在了1号NUMA节点上，所以irqbalance把中断分配给了1号NUMA节点的CPU，该网卡缺省有8个收发队列，每个队列对应的中断被分配给一个CPU，所以表现出来的症状是8个CPU的软中断率很高。

#### 解决过程

https://mp.weixin.qq.com/s/1p9eUYP6pZK4im-vSCnb1Q

将网卡队列从8调整到16，每个core的si降低了，但是 usr%相应提高了不少。node1的CPU仍然接近100%

知识：中断所唤醒的进程倾向于运行在中断所在的NUMA节点上

同时发现有个第三方的网络过滤模块消耗CPU过高，停掉这个模块就平稳度过了黑色星期五。

进一步分析原因

https://mp.weixin.qq.com/s/AzcB1DwqRCoiofOOI88T9Q

1号NUMA节点的CPU使用率超高与数据库服务变慢有没有直接关系？毕竟这台服务器有8个NUMA节点，其余7个节点的CPU都没那么忙。

因为1号Node 打满导致上面的进程处理缓慢；为什么不将上面的进程调度到其它Node，因为wakeup affinity

![image-20220707151642981](/Users/ren/src/blog/951413iMgBlog/image-20220707151642981.png)

![image-20220707151656456](/Users/ren/src/blog/951413iMgBlog/image-20220707151656456.png)

https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA

Linux的进程调度有一个不太为人熟知的特性，叫做**wakeup affinity**，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。

这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。

![Image](/Users/ren/src/blog/951413iMgBlog/640-7178470.png)

https://mp.weixin.qq.com/s/yY9_SjtBL4hPz6_XM82Lhg

https://mp.weixin.qq.com/s/iWpsnUFJ1_T09U6cfOLGTw

![Image](/Users/ren/src/blog/951413iMgBlog/640-20220707152145610.png)

#### 总结

首先要搞清楚NUMA结构，然后看网卡插在哪个node上（从文章中看是插在node0上）

然后将软中断队列绑在node0 上，数据库进程绑在1-7 node上

![image-20220707154232470](/Users/ren/src/blog/951413iMgBlog/image-20220707154232470.png)

保证上图中的 QP(Queue Pair) 和 Data Buffer 不要跨node



